Project Description
CS-449: Systems for Data Science


BACKGROUND

In this course, our focus is on the systems perspective of managing and analysing large amounts of data. We expect you to be familiar with basic concepts of machine learning, specifically, we assume that you are familiar with Stochastic Gradient Descent (SGD) and how it can be used for learning Support Vector Machines (SVM). These are popular topics on the Web, where you can find plenty of example implementations.

In the context of big data, a single-threaded version of SGD is often not fast enough. There are many ways to implement a multi-threaded version of SGD. However, it is not very straightforward to implement an efficient parallel version of SGD due to its inherently sequential nature. HOGWILD! (Niu et al., Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, easy to find with Google) is a parallel implementation of SGD that achieves good performance. The key insight behind this work is that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, it is tolerable to let threads overwrite each otherâ€™s work (it is not unlike noisy data), and we can still achieve a good rate of convergence. We expect you to read this paper and understand the authors' insight and how they achieved such good performance. You can skip the appendix (in fact, most of what you need to get from the paper is in Section 2). 


OVERVIEW & GOAL

In this project, your task is to implement two efficient distributed versions of SGD (one synchronous and one asynchronous), with a maximally shared codebase, which will be used to train an SVM. Please note this is different from HOGWILD!, which is running on a single multicore machine and uses multiple threads that communicate with each other using shared memory. Your program will be running on multiple machines, which communicate with each other by sending/receiving messages through the network. 

During this semester, we will cover systems with synchronous and asynchronous aspects in a number of contexts, but it must be made clear that "synchronous" can mean more than one thing in this context. It may mean an implementation in which the state of the vector to be learned after each iteration of (stochastic) gradient descent is the same on all machines, or it may mean that this state is in addition equivalent to that of a single-threaded implementation ("serializable"). By an asynchronous implementation, we refer to one that takes the idea of HOGWILD! and analogously carries it over to a distributed systems context. You need to think for yourself what that means. There may be more than one possible way of solving this that we will accept; you will have to be able to justify and defend your choices.

The project is to be carried out in groups of THREE people (in exceptional cases, teams of two people will be acceptable). 

The project has two milestones. 


MILESTONE 1 - SYNCHRONOUS DISTRIBUTED SGD (moodle upload due April 16 11:59pm; meeting with the TA in that week)

For the first milestone, we expect you to deliver an implementation of SVM using distributed synchronized SGD.

In this implementation, you need to have a coordinator, whose job is to set up worker machines by copying the required files to all the machines and starting the program. The coordinator should read a configuration file describing the worker nodes, the dataset, and any other configuration options. The worker machines are the ones who run the actual computation. The workers share their computed parameters after each step and wait for updates from all their peers.

You are free to implement your project using any programming language you prefer. You may use libraries to facilitate the reading of formatted (eg. CSV) file. However, you cannot use calls to any high-level tool or library that does SGD for you. You need to implement the core learning algorithm yourself. For the communication part, you must use gRPC and Google Protocol Buffers, which automatically generate idiomatic client and server stubs for your service in the programming language of your choice (there are very nice tutorials for gRPC/protobuf online). We advise you to first implement a single-threaded version of the algorithm and play with it a bit to make sure you got it right and understood how things work. You should then extend your single-threaded version to implement the distributed version of the algorithm.

The dataset we recommend you to use for testing your system is Reuters Corpus Volume I (RCV1), which is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes (easy to find using Google). This is the dataset that was used in the HOGWILD! paper. For Milestone 1, your correctness testing is to happen on your own machines, by running multiple worker instances on the same machine if necessary. For Milestone 2, we will provide you with resources to properly run your implementation in a distributed fashion.


DELIVERABLES AND DEADLINES FOR MILESTONE 1

You should find your team of three people as soon as possible; We strongly suggest to complete this in the coming two weeks, and to use lecture breaks and the exercise sessions to look for team members. If you have been able to form a team of two and are looking for a third person, you may use the course forum on moodle to call for a third team member.

The deadline for the first milestone is April 16th, and you will be asked to upload a single zip file on moodle containing your implementation of SVM using distributed synchronous SGD before April 16th 11:59pm. It is sufficient for one team member to upload the code; the zip file should contain a text file README.txt which lists the names of all team members.

We will inform you in time about how to sign up for a meeting with a TA to defend your submission, which will take place in the week of April 16-20.


MILESTONE 2 preview (A detailed Milestone 2 spec will be posted right after the due date of Milestone 1)

For the second milestone, which will be due in the week before the final exam, you have to extend the capabilities of your program to do the communication in an asynchronous fashion. In this case, the workers do not block on sending/receiving updates to/from their peers, and they directly continue the computation after sending their own updates. We want your implementation to support both synchronous and asynchronous distributed SGD with a maximally shared codebase, so you should plan this ahead of time. There should be an option in the configuration file passed to the coordinator that enables the user to specify the type of the communication, i.e., synchronous or asynchronous.

We will ask you to submit a report describing what you did, how you implemented the algorithm, how you chose the learning rates and stopping criteria, along with a comparison between your choice and the choices made in the paper. We will also ask you to run some experiments with different setups, report your findings and discuss them. Similarly to the first milestone, there will be a session in which you will present and defend your work in front of the TAs, and you will be questioned about what you did, how you implemented the project, and about the contents of your report.


GRADING OF THE PROJECT (30% of the course grade)

Milestone 1:  5%
Milestone 2: 25%


NOTES

The purpose of Milestone 1 is primarily to give you a (gentle) push to get going and not to leave all work to the final weeks of the term, and to give you a chance to get feedback on your work done so far by a TA. Milestone 1 carries relatively little weight in the grade and the grading is entirely based on the meeting with the TA. 

There is a publicly available implementation of HOGWILD!. You may think it a good idea to take and modify this implementation, but we warn you that this will be very hard, since this is a highly optimized low-level implementation that does many things that have no corresponding meaning in a distributed implementation; plus the code is large, hard to read, and to understand. There are, however, single-core implementations of SGD for SVM on the Web, which you may start from. If you do, make sure to indicate clearly in your submission that you did this, where you obtained the implementation you started from, and include a copy of it in unmodified form in your submission. This will be acceptable. However:


CHEATING

Your team may NOT share or exchange any fragment of the code you developed with other teams. While you are of course permitted to talk about the contents of the course with other students and learn together, be warned that, should we find significant overlap between the solutions of different teams, excuses such as "we only talked about it" will not be accepted.

